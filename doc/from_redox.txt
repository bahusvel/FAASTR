The kernel began life from redox tag version 0.3.5, it should be included in this repo, but if ever it is lost its important.

Notes on reverse engineering redox's code:

* arch/x86_64/pti is a page table isolation feature (unmaps kernel pages) to help against meltdown attack. This is not essential and it seems to only do stuff with stack and otherwise seems useless.

* BSP is the first processor and APs are secondary/auxilary processors.

Understanding memory subsystem:

* The bootloader creates a map of physical memory areas, they consist of actual memory and I guess also devices. Some of these memory areas are free, marked with type 1. Other are occupied, apparently there are some that are not available too, I'm guessing that's just unused address space. Its not associated an ACPI, those are ACPI flags which describe if some extended attributes that are basically undefined right now except for non volatile which is bit 1.

* its interesting because it says if bit 0 is clear then I shouldnt use that memory, but I also guess this has to do with ACPI version. Because apparently ACPI version 3.0 only does that. But perhaps the emulator is not emulating 3.0?

* So the bootloader is getting the memory area map from the bios by using BIOS Function: INT 0x15, EAX = 0xE820 as described here: https://wiki.osdev.org/Detecting_Memory_(x86)

MemoryArea { base_addr: 0, length: 654336, _type: 1, acpi: 0 }
MemoryArea { base_addr: 654336, length: 1024, _type: 2, acpi: 0 }
MemoryArea { base_addr: 983040, length: 65536, _type: 2, acpi: 0 }
MemoryArea { base_addr: 1048576, length: 2146299904, _type: 1, acpi: 0 }
MemoryArea { base_addr: 2147348480, length: 135168, _type: 2, acpi: 0 }
MemoryArea { base_addr: 2952790016, length: 268435456, _type: 2, acpi: 0 }
MemoryArea { base_addr: 4275159040, length: 16384, _type: 2, acpi: 0 }
MemoryArea { base_addr: 4278173696, length: 16384, _type: 2, acpi: 0 }
MemoryArea { base_addr: 4294705152, length: 262144, _type: 2, acpi: 0 }

* Studying the memory areas above, there is a large chunk of roughly 2046 MB that is free, this is the majority of free memory supplied to the VM. There is another free chunk at the beginning, I'm guessing that is some kind of buffering or where the bootloader was or something. Another large chunk can be observed of 256MB, but it is marked as used. Just as all other ones, I'm gessing they are memory mapped devices. Most of which are rather small.

* Upon further reading from here https://wiki.osdev.org/Detecting_Memory_(x86), it appears that the memory area 1 and 2 (counting from zero) are not reclaimable, they contain some chipset specific magic, used by SMM, and bios, and all sorts of cool and evil stuff. So the very next area is actually free free RAM! And hence they are not contiguous.

* I see, so according to this 100000:20B260, the kernel is also loaded into that big chunk of free memory area. And appears to be roughly a MB in size? That is why this information is passed into the allocator, so that the allocator doesn't go allocating that memory willy nilly.

* There is a frame allocator, it is stacked, bump + recycle, bump cannot free frames, recycle keeps all free frame in a vector, and it will iterate through them. Which sounds like a bad idea. When the frame is not in use, it will go to the recycle allocator vector.

* The bump allocator just allocates frames from an iterator of free memory areas. That's all. I wonder why would there be multiple free memory areas? (Except for the first one and the major one).

* I see, the recycle allocator uses a vector, which should allocate memory, but virtual memory subsystem is not even up yet! They rely on the fact that Vec::new() doesnt allocate, there is a flag that checks this, redox refers to it as core. Once core is there it can do its reclaiming and allocating business. Thats all very dodgy dangerous and basically unrestricled. Althought maybe the allocator would panic because its not set yet? Yes it will.

* After paging starts, gdt is setup, it will setup the allocator, the heap, and then enable noncore of the frame allocator which will allow it to use that vec.

Understanding the paging:

* Paging initialisation gets parameters about various parts of the kernel. Including its physical location and virtual addresses of the code. So virtual memory subsystem is already definetly up. And kernel is mapped to 0xffffff0000100000 according to the linker script. It's stack is FFFFFF0000080000 - FFFFFF000009F000 and some environment thingy there as well. The initialiser for paging also retrieves addresses of various memory areas like bss, tbss, data, text, rodata and so on from labels provided by the compiler/linker. Smart. I'm guessing it will use those to create protection.

* The offset is kinda cool, because 0x100000  is the physical address of the beggining of the kernel. So to map it to virtual address one just needs to append 0xffffff0000.

* First of all paging initialises PAT, which is table stored in register that allows us to specify on per page basis what kind of caching to use. It then creates page table structure and makes in inactive, it then proceeds to fill up that page table structure with kernel mappings. Once this page table structure is created, it will switch to it. (And because it matches current structure) thats alright.

* The mapper is very nice and mostly transparent it just manages the kary (4 level) page table, where it creates and removes entries of page mappings, it can allocate frames by itself which is nice, or can be given pre-allocated or re-used frames which is also nice. The unmap_inner function is written very weirdly, but the logic is reasonably straighforward.

* Then the GDT is setup which tells the CPU what each segment means. Also quite complex may need to figure out what it is and what it does later one. Basically this is just to define all the segment selectors. I'm pretty sure its not that interesting, but I man encounter GDT or LDT in context switching, in which case it might be slightly important.

Understaning kernel heap allocation:

* Up to this point there is basically no heap allocation in the kernel, any attempts would panic. The arch startup will now initialise the heap allocator. Which seems reasonably simple. It maps the heap at some known predefined offset. The allocation itself happens in the mapper. And it will allocate 1MB at a time. And will increase the heap size only if allocation fails. Seems pretty straighforward to me, which is very nice.

* How does the allocator get set as Rusts default allocator? Probably somewhere in the root of the crate. Yes, here:
#[global_allocator]
static ALLOCATOR: allocator::Allocator = allocator::Allocator;

* ACPI Stuff, I believe it is most probably only relevant for AP bring up, the rest is for devices and some system config stuff.

Understanding contexts:

* The whole context magic appears to stem from the main kernel function, and where contexts are initialised. That is context::init(). This function is called once per core, and it's purpose is to create a kernel context per core.

* Important aspect of the context is the list of contexts on the system. This list is global and shared between all calls. New contexts are created as elements of that list. Although the list is called a list its actually a BTreeMap, but regarless it can still be sequential. A context is always created at the end of the day through ContextList::new_context() method, which inserts the context, and gives a reference to it. context::init() function creates the initial context for each core.

* Each core has a thread local variable which identifies the context it is currently in. CONTEXT_ID.

* Otherwise this function just does some very very trivial initialisation like setting this context to runnable and running, setting the cpu, allocating memory for fx stuff storage.

* The fact that the list of context is a singular global locked datastructure is kinda bad. It is RwLock datastructure but even then it can still cause starvation.

This is where context init basically ends, further points of entry into context management are through execution of code and commands. One such is context::contexts_mut().spawn(userspace_init). Which uses ContextList::spawn() to run a function in kernel space in a separate context.

Spawning of context:

* It begins with a creation of context in a standard manner with new_context(). Allocation of fx stuff, and also a stack for this function to run. The stack is 64k, and right at the beggining stack (end of memory) we put a function pointer that we would like to execute (I guess it so so we can run it by executing ret).

* Then we create an active page table, and set it into the arcithecture dependent portion of the context, but this page table is empty. No? Why is it empty? Perhaps we will find out soon enough.

* The rest is fairly simple we set the stack and the fx variables into the context, and then return a handle to it.

* We are not running this function yet, this would occur with context::switch(), what's weird is that we disable interrupts before calling context::switch(), why? Perhaps context switching is not interrupt safe, but if so, why not put it into the context::switch method itself?

* Man the scheduling is really fucky, first it will obtain some global lock for everything in the world. It will find current context (which is fine), traverse the list of context updating all of them. Then it will traverse the list again (from current context) looking for a task to run, if it doesnt find anything it will traverse the list from beggining to current task looking for task to run. The first traversal seems like best try to get everything to update. The rest seems like trying to make round robin fair.

* It will then switch current context to not running, the new context to running, and setup the tss. It will drop the global context switch lock (not sure why). Run signal handler if there is one and otherwise trigger architecture specific switching routine.
